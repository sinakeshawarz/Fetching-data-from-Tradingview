# Install the required library from GitHub in Google Colab (using the active fork with TvDatafeed support)
!pip install --upgrade --no-cache-dir git+https://github.com/rongardF/tvdatafeed.git

# Import necessary modules
from tvDatafeed import TvDatafeed, Interval
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import time  # For retry sleep

# Initialize TvDatafeed in no-login mode (avoids failed login attempts; data may be limited)
tv = TvDatafeed()
print("Using no-login mode; some symbols may be restricted.")

# Define the symbols and exchanges (updated per user notes: BBG for BCOM, FX_IDC for USDBRL/ICE)
symbols = {
    'BCOM': {'symbol': 'BCOM', 'exchange': 'BBG'},
    'CCM1!': {'symbol': 'CCM1!', 'exchange': 'BMFBOVESPA'},
    'USDBRL': {'symbol': 'USDBRL', 'exchange': 'FX_IDC'}
}

# Function to fetch historical data from 2021 to today with retry logic
def fetch_historical_data(symbol, exchange, interval=Interval.in_daily, n_bars=5000, start_date='2021-01-01', retries=3):
    for attempt in range(retries):
        try:
            data = tv.get_hist(symbol=symbol, exchange=exchange, interval=interval, n_bars=n_bars)
            if data is not None and not data.empty:
                # Filter to the desired date range
                data = data[data.index >= pd.to_datetime(start_date)]
                data = data[data.index <= pd.to_datetime(datetime.now().date())]
                if not data.empty:
                    print(f"\nHistorical data for {symbol} on {exchange} ({interval.name} interval) from {start_date} to today:")
                    print(data.tail(10))  # Display last 10 rows for verification
                    return data
                else:
                    print(f"No data in the specified range for {symbol} on {exchange}.")
                    return None
            else:
                print(f"No historical data retrieved for {symbol} on {exchange} (attempt {attempt+1}/{retries}).")
        except Exception as e:
            print(f"Error fetching for {symbol} (attempt {attempt+1}/{retries}): {str(e)}")
        time.sleep(5)  # Wait before retry
    print(f"Failed to fetch data for {symbol} after {retries} attempts.")
    return None

# Main logic: Fetch historical data from 2021-today, save to CSV, plot, and prepare for analysis
historical_data = {}

for key, info in symbols.items():
    symbol = info['symbol']
    exchange = info['exchange']
    
    print(f"\nProcessing {key}: {symbol} on {exchange}")
    
    # Fetch with retries
    hist = fetch_historical_data(symbol, exchange)
    if hist is not None:
        historical_data[key] = hist
        
        # Save to CSV in Colab's /content directory for further analysis/download
        csv_path = f'/content/{key}_historical_data_2021_to_{datetime.now().date()}.csv'
        hist.to_csv(csv_path)
        print(f"Saved historical data to {csv_path}")
        
        # Plot the close prices
        plt.figure(figsize=(12, 6))
        hist['close'].plot()
        plt.title(f"{key} Daily Close Prices (2021 to Today)")
        plt.xlabel("Date")
        plt.ylabel("Close Price")
        plt.grid(True)
        plt.show()



# Convert CCM1! prices to USD/ton using USDBRL exchange rate
# Add this section right after the fetching loop (after 'historical_data = {}' block) and before the multi-symbol plot
if 'CCM1!' in historical_data and 'USDBRL' in historical_data:
    ccm_df = historical_data['CCM1!'].copy()
    usdbrl_df = historical_data['USDBRL'].copy()
    
    # Normalize indexes to date-only for alignment (remove time component)
    ccm_df.index = ccm_df.index.normalize()
    usdbrl_df.index = usdbrl_df.index.normalize()
    
    # Merge on common dates
    combined = pd.merge(ccm_df[['open', 'high', 'low', 'close', 'volume']], 
                        usdbrl_df[['close']], 
                        left_index=True, right_index=True, 
                        suffixes=('', '_usdbrl'))
    
    # Convert to USD/ton: (BRL per 60kg bag * (1000kg/60kg)) / USDBRL
    bags_per_ton = 1000 / 60
    for col in ['open', 'high', 'low', 'close']:
        combined[f'{col}_usd_ton'] = (combined[col] * bags_per_ton) / combined['close_usdbrl']
    
    # Update historical_data['CCM1!'] with converted prices (overwrite OHLC, keep volume)
    historical_data['CCM1!'] = combined[['open_usd_ton', 'high_usd_ton', 'low_usd_ton', 'close_usd_ton', 'volume']]
    historical_data['CCM1!'].rename(columns={
        'open_usd_ton': 'open',
        'high_usd_ton': 'high',
        'low_usd_ton': 'low',
        'close_usd_ton': 'close'
    }, inplace=True)
    
    print("Converted CCM1! prices to USD/ton using USDBRL exchange rates.")
else:
    print("Skipping CCM1! conversion: Required data (CCM1! and/or USDBRL) not available.")

    
# Optional: Multi-symbol plot if multiple fetched
if len(historical_data) > 1:
    plt.figure(figsize=(12, 6))
    for key, data in historical_data.items():
        data['close'].plot(label=key)
    plt.title("Multi-Symbol Daily Close Prices (2021 to Today)")
    plt.xlabel("Date")
    plt.ylabel("Close Price")
    plt.legend()
    plt.grid(True)
    plt.show()

# Optional: Enhanced analysis (with correlations if multiple symbols)
if historical_data:
    print("\nEnhanced Analysis of Historical Data:")
    for key, data in historical_data.items():
        if data is not None and not data.empty:
            print(f"\nAnalysis for {key}:")
            data['SMA_20'] = data['close'].rolling(window=20).mean()
            data['SMA_50'] = data['close'].rolling(window=50).mean()
            data['SMA_200'] = data['close'].rolling(window=200).mean()  # Long-term trend
            recent_close = data['close'].iloc[-1]
            recent_volume = data['volume'].iloc[-1]
            avg_volume = data['volume'].mean()
            returns = data['close'].pct_change().dropna()
            volatility = returns.std() * (252 ** 0.5)  # Annualized volatility
            print(f"Most recent close: {recent_close:.2f}")
            print(f"Most recent volume: {recent_volume:.2f}")
            print(f"Average volume: {avg_volume:.2f}")
            print(f"Last 20-day SMA: {data['SMA_20'].iloc[-1]:.2f}")
            print(f"Last 50-day SMA: {data['SMA_50'].iloc[-1]:.2f}")
            print(f"Last 200-day SMA: {data['SMA_200'].iloc[-1]:.2f}")
            print(f"Annualized volatility: {volatility:.2%}")
            # Enhanced trend evaluation
            if data['close'].iloc[-1] > data['SMA_20'].iloc[-1] > data['SMA_50'].iloc[-1] > data['SMA_200'].iloc[-1]:
                print("Trend: Strongly Bullish (close > 20-SMA > 50-SMA > 200-SMA)")
            elif data['close'].iloc[-1] > data['SMA_20'].iloc[-1] > data['SMA_50'].iloc[-1]:
                print("Trend: Bullish (close > 20-SMA > 50-SMA)")
            elif data['close'].iloc[-1] < data['SMA_20'].iloc[-1] < data['SMA_50'].iloc[-1] < data['SMA_200'].iloc[-1]:
                print("Trend: Strongly Bearish (close < 20-SMA < 50-SMA < 200-SMA)")
            elif data['close'].iloc[-1] < data['SMA_20'].iloc[-1] < data['SMA_50'].iloc[-1]:
                print("Trend: Bearish (close < 20-SMA < 50-SMA)")
            else:
                print("Trend: Neutral or Consolidating")
    
    # Correlations if multiple symbols
    if len(historical_data) > 1:
        closes = pd.DataFrame({key: data['close'] for key, data in historical_data.items()})
        corr_matrix = closes.corr()
        print("\nCorrelation Matrix:")
        print(corr_matrix)
else:
    print("No historical data available for analysis.")


# Professional Correlation Analysis (Pairwise on Aligned Time Series)
if len(historical_data) >= 2:
    print("\nProfessional Correlation Analysis (Pairwise on Aligned Time Series):")
    
    # For overall matrix (computed later if possible)
    all_closes = pd.DataFrame()
    for k, v in historical_data.items():
        s = v['close'].copy()
        s.index = s.index.normalize()  # Normalize to 00:00:00
        all_closes[k] = s
    all_closes = all_closes.dropna(how='any')  # Common dates only
    
    pairs = [(a, b) for a in historical_data for b in historical_data if a < b]  # Unique pairs
    
    for pair in pairs:
        sym1, sym2 = pair
        
        # Per-pair alignment
        df1 = historical_data[sym1]['close'].copy()
        df2 = historical_data[sym2]['close'].copy()
        df1.index = df1.index.normalize()
        df2.index = df2.index.normalize()
        
        # Align on common dates with inner join
        closes_pair = pd.concat([df1, df2], axis=1, keys=[sym1, sym2]).dropna(how='any')
        
        if closes_pair.empty or len(closes_pair) < 2:
            print(f"Insufficient overlapping data for pair {sym1} vs. {sym2}.")
            continue
        
        # Reindex to full daily range with ffill
        min_date = closes_pair.index.min()
        max_date = closes_pair.index.max()
        full_dates = pd.date_range(start=min_date, end=max_date, freq='D')
        closes_pair = closes_pair.reindex(full_dates).ffill().dropna()
        
        returns_pair = closes_pair.pct_change().dropna()
        
        if len(returns_pair) < 2:
            print(f"Insufficient data after returns calculation for pair {sym1} vs. {sym2}.")
            continue
        
        x = returns_pair[sym1].values
        y = returns_pair[sym2].values
        x_level = closes_pair[sym1].values
        y_level = closes_pair[sym2].values
        
        print(f"\nPair: {sym1} vs. {sym2}")
        
        # Static Correlations (on returns)
        pearson_r, pearson_p = stats.pearsonr(x, y)
        spearman_r, spearman_p = stats.spearmanr(x, y)
        kendall_r, kendall_p = stats.kendalltau(x, y)
        print("Static Correlations (on returns):")
        print(f"  Pearson: r={pearson_r:.4f}, p-value={pearson_p:.4f}")
        print(f"  Spearman (non-linear rank): r={spearman_r:.4f}, p-value={spearman_p:.4f}")
        print(f"  Kendall (ordinal association): tau={kendall_r:.4f}, p-value={kendall_p:.4f}")
        
        # Mutual Information (non-linear dependency on levels)
        mi = mutual_info(x_level, y_level)
        print(f"  Mutual Information (non-linear): {mi:.4f}")
        
        # DTW Similarity (pattern matching; lower distance = higher similarity)
        dtw_dist = dtw(x_level, y_level)
        dtw_sim = 1 / (1 + dtw_dist)  # Normalized similarity [0,1]
        print(f"  DTW Distance: {dtw_dist:.4f}, Similarity: {dtw_sim:.4f}")
        
        # Rolling Correlation (Pearson on returns, 252-day window ~1 year)
        rolling_corr = returns_pair[sym1].rolling(window=252).corr(returns_pair[sym2]).dropna()
        print(f"  Rolling Correlation (252-day): Mean={rolling_corr.mean():.4f}, Std={rolling_corr.std():.4f}")
        plt.figure(figsize=(12, 6))
        rolling_corr.plot()
        plt.title(f"Rolling 252-day Correlation: {sym1} vs. {sym2}")
        plt.xlabel("Date")
        plt.ylabel("Correlation")
        plt.grid(True)
        plt.show()
        
        # Cross-Correlation (lags; max lag=30 days)
        cross_corrs = ccf(x, y, adjusted=False)[:31]  # First 30 lags + lag0
        max_lag = np.argmax(np.abs(cross_corrs[1:])) + 1  # Optimal non-zero lag
        print(f"  Cross-Correlation (optimal lag): {max_lag} days, corr={cross_corrs[max_lag]:.4f}")
        plt.figure(figsize=(12, 6))
        plt.stem(range(len(cross_corrs)), cross_corrs)
        plt.title(f"Cross-Correlation Function: {sym1} vs. {sym2}")
        plt.xlabel("Lag (days)")
        plt.ylabel("Correlation")
        plt.grid(True)
        plt.show()
        
        # Novel Procedure: Bidirectional Optimal Lag Search for Maximum Absolute Correlation
        # Computes correlations over -30 to +30 lags; positive lag means sym1 leads sym2
        nlags = 30
        pos_corrs = ccf(x, y, adjusted=False)[:nlags+1]  # lag 0 to +30 (sym1 leads)
        neg_corrs = ccf(y, x, adjusted=False)[1:nlags+1][::-1]  # lag -1 to -30 (sym2 leads), flipped
        full_corrs = np.concatenate((neg_corrs, pos_corrs))
        full_lags = np.arange(-nlags, nlags+1)
        abs_corrs = np.abs(full_corrs)
        opt_idx = np.argmax(abs_corrs)
        opt_lag = full_lags[opt_idx]
        opt_corr = full_corrs[opt_idx]
        print(f"  Optimal Lag for Max Abs Correlation: lag={opt_lag} days, corr={opt_corr:.4f}")
        if opt_lag > 0:
            print(f"    Interpretation: {sym1} leads {sym2} by {opt_lag} days at peak correlation.")
        elif opt_lag < 0:
            print(f"    Interpretation: {sym2} leads {sym1} by {-opt_lag} days at peak correlation.")
        else:
            print(f"    Interpretation: Contemporaneous peak correlation (no lead/lag).")
        plt.figure(figsize=(12, 6))
        plt.stem(full_lags, full_corrs)
        plt.title(f"Bidirectional Cross-Correlation: {sym1} vs. {sym2}")
        plt.xlabel("Lag (days; positive: sym1 leads)")
        plt.ylabel("Correlation")
        plt.grid(True)
        plt.show()
        
        # Cointegration Analysis (using Engle-Granger two-step method for simplicity and efficiency)
        # Step 1: Regress levels and test residuals for stationarity (ADF test)
        if len(closes_pair) > 10:  # Sufficient data
            try:
                # OLS regression: sym2 ~ sym1 (or vice versa; we test both directions for robustness)
                X = sm.add_constant(closes_pair[sym1])
                model = sm.OLS(closes_pair[sym2], X).fit()
                residuals = model.resid
                adf_res = sm.tsa.adfuller(residuals, regression='c')  # ADF test on residuals (constant only)
                adf_stat = adf_res[0]
                adf_pval = adf_res[1]
                print(f"  Cointegration (Engle-Granger):")
                print(f"    ADF Statistic on Residuals: {adf_stat:.4f}, p-value={adf_pval:.4f}")
                if adf_pval < 0.05:
                    print("      Evidence of cointegration (residuals stationary at 5% level) - long-run equilibrium exists.")
                else:
                    print("      No evidence of cointegration (residuals non-stationary).")
                
                # Reverse direction for comparison
                X_rev = sm.add_constant(closes_pair[sym2])
                model_rev = sm.OLS(closes_pair[sym1], X_rev).fit()
                residuals_rev = model_rev.resid
                adf_res_rev = sm.tsa.adfuller(residuals_rev, regression='c')
                adf_stat_rev = adf_res_rev[0]
                adf_pval_rev = adf_res_rev[1]
                print(f"    Reverse Direction ADF: Stat={adf_stat_rev:.4f}, p-value={adf_pval_rev:.4f}")
                
                # Plot residuals for visual inspection
                plt.figure(figsize=(12, 6))
                plt.plot(closes_pair.index, residuals, label='Residuals')
                plt.title(f"Cointegration Residuals: {sym2} ~ {sym1}")
                plt.xlabel("Date")
                plt.ylabel("Residuals")
                plt.legend()
                plt.grid(True)
                plt.show()
            except Exception as e:
                print(f"  Cointegration test failed: {str(e)}")
        else:
            print("  Insufficient data for cointegration analysis.")
        
        # Granger Causality (predictive; max lag=5, on differenced for stationarity)
        if len(returns_pair) > 10:  # Need sufficient data for Granger (maxlag+1 min)
            diff_returns = returns_pair.diff().dropna()  # Ensure stationarity
            granger_data = diff_returns[[sym1, sym2]].values
            granger_res = grangercausalitytests(granger_data, maxlag=5, verbose=False)
            granger_pvals = [granger_res[lag+1][0]['ssr_ftest'][1] for lag in range(5)]  # p-values
            min_p = min(granger_pvals)
            best_lag = granger_pvals.index(min_p) + 1
            print(f"  Granger Causality ({sym1} -> {sym2}): Min p-value={min_p:.4f} at lag={best_lag}")
            # Reverse direction
            granger_data_rev = diff_returns[[sym2, sym1]].values
            granger_res_rev = grangercausalitytests(granger_data_rev, maxlag=5, verbose=False)
            granger_pvals_rev = [granger_res_rev[lag+1][0]['ssr_ftest'][1] for lag in range(5)]
            min_p_rev = min(granger_pvals_rev)
            best_lag_rev = granger_pvals_rev.index(min_p_rev) + 1
            print(f"  Granger Causality ({sym2} -> {sym1}): Min p-value={min_p_rev:.4f} at lag={best_lag_rev}")
        else:
            print("  Insufficient data for Granger Causality tests.")
    
    # Overall Correlation Matrix (Pearson on returns), if common data available
    if not all_closes.empty:
        all_returns = all_closes.pct_change().dropna()
        if not all_returns.empty:
            corr_matrix = all_returns.corr()
            print("\nOverall Pearson Correlation Matrix (on returns):")
            print(corr_matrix)
        else:
            print("\nNo sufficient data for overall correlation matrix.")
    else:
        print("\nNo common dates for all symbols; pairwise analyses only.")
else:
    print("Insufficient symbols fetched for correlation analysis (need at least 2).")
